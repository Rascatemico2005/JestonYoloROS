https://github.com/Rascatemico2005/JestonYoloROS/releases

# JetsonYoloROS: Real-time YOLO with TensorRT on ROS for Jetson

[![GitHub release](https://img.shields.io/github/v/release/Rascatemico2005/JestonYoloROS)](https://github.com/Rascatemico2005/JestonYoloROS/releases) [![ROS Noetic / Melodic](https://img.shields.io/badge/ROS-Noetic%20%7C%20Melodic-blue)](https://www.ros.org) [![JetPack](https://img.shields.io/badge/JetPack-4.x-blue)](https://developer.nvidia.com/jetson-platform) [![TensorRT](https://img.shields.io/badge/TensorRT-8.x-orange)](https://developer.nvidia.com/tensorrt) [![CUDA](https://img.shields.io/badge/CUDA-11.x-blue)](https://developer.nvidia.com/cuda-zone)

Introduction
This project brings real-time object detection to Nvidia Jetson devices using the ROS framework. It leverages TensorRT and CUDA to accelerate YOLO inference on embedded hardware. The system is designed to be modular and easy to extend, so developers can plug in multiple YOLO variants and customize the input and output streams to fit their robotics or edge AI workflows.

What you will find here
- A ROS-ready package that connects a camera stream to a TensorRT-accelerated YOLO detector
- A pipeline that runs inference on the Jetsonâ€™s GPU, compiles optimized engines, and publishes detections as ROS messages
- Sample launch files and configuration utilities to switch between models and input sources
- Guidance for performance optimization on Jetson devices, with practical tips for power and timing
- Clear, approachable documentation that helps you adapt the system to your robots or edge devices

This repository centers on making YOLO fast enough for real-time perception on Jetson hardware and on keeping the integration with ROS clear and maintainable. It aims to lower the barrier to adopting modern computer vision in mobile robots and edge devices.

Why this project matters
Jetson devices pack a capable GPU in a small form factor. They run ROS well and fit many robot platforms. YOLO offers strong accuracy and speed for object detection. TensorRT accelerates inference on Nvidia hardware. By combining these pieces, developers can add reliable vision to robots without a heavy compute payload.

Structure of this README
- Getting started with minimal friction
- How the system works under the hood
- Step-by-step installation from prebuilt releases
- Building from source for custom setups
- How to use and customize the pipeline
- Troubleshooting and performance guidance
- Contributing and licensing information

Where this fits in your project
This project is designed for small to mid-size robots, drones, and edge devices that need fast object detection without a powerful desktop GPU. It is a good fit when you need ROS-based perception, a proven YOLO model, and hardware-accelerated inference on Jetson devices.

Quick start overview
- Identify your Jetson device and ROS setup
- Download the appropriate release from the Releases page
- Install the package and run a simple demo
- Verify detections on a live camera or video stream
- Tune models and engine precision to match your latency and accuracy needs

Releases and how to obtain the software
The repository provides prebuilt assets as part of its releases. These assets are designed to install quickly on Jetson devices and run the detector with minimal setup. The Releases page includes engine binaries, ROS packages, and convenience scripts to simplify deployment. To explore the available assets, visit the Releases page and download the file that matches your hardware. The formula is simple: download, extract, and run the installer script included in the asset. The link you need is the Releases page, and you will use it again later in this document to point to the same resource.

First steps for new users
1) Confirm your hardware and software versions
- Jetson model: Nano, TX2, Xavier NX, or AGX Xavier
- JetPack version: 4.x (typical for many Jetson deployments)
- ROS: Noetic on Ubuntu 20.04 or Melodic on Ubuntu 18.04 (or their ROS 1 equivalents)
- CUDA toolkit and TensorRT runtime versions aligned with JetPack
2) Prepare your Jetson
- Flash the Jetson with the target JetPack and enable the CUDA and TensorRT runtimes
- Ensure network access and a stable power source for consistent inference
3) Acquire the release asset
- Go to the Releases page and download the Linux aarch64 tarball that matches your Jetson model
- You will find an installer script and a ROS package inside the archive
4) Run the installer
- Extract the archive and run the included installer script
- The installer will set up the ROS package, download or prepare the engine, and configure sample launch files
5) Run a quick demo
- Start your ROS core
- Launch the sample pipeline with a test camera
- Observe detections published on standard ROS topics

Downstream goals and use cases
- Real-time object detection in mobile robots
- Visual servoing and navigation with obstacle awareness
- Monitoring and safety due to live detections
- Edge AI inference in remote locations with limited bandwidth
- Prototyping YOLO-based perception pipelines for research and education

What you should know before using this project
- This project relies on Nvidia Jetson hardware and software ecosystems
- It uses TensorRT to accelerate YOLO inference
- It is built to work inside the ROS landscape, either ROS 1 or ROS 2 with careful adaptation
- The primary focus is inference speed and integration, not training
- You should have a working ROS workspace and familiarity with ROS launch files

Getting to know the architecture
- Camera input module: subscribes to a camera image stream or video feed
- Preprocessing: formats input for YOLO, resizes images, and handles color channels
- Inference engine: runs a TensorRT-optimized YOLO model on the Jetson GPU
- Postprocessing: decodes YOLO outputs into bounding boxes, labels, and confidence scores
- ROS interface: publishes detections, subscribes to control topics, and provides a simple API to interact with the pipeline
- Optional visualization: real-time bounding boxes on visuals and optional RViz integration

What is YOLO in this context
- YOLO is a family of fast, single-shot detectors
- In this project, a TensorRT-optimized version runs on Jetson hardware
- You can swap YOLO variants as needed (e.g., YOLOv4-tiny or YOLOv5 variants) to balance speed and accuracy
- The system is designed to load a model and its associated TensorRT engine at runtime

How the system handles models and engines
- Models are converted to TensorRT engines for fast inference
- Engines are cached to avoid repeated compilation
- You can switch models by loading a different engine and updating config files
- Precision modes include FP16 for speed with manageable accuracy, and FP32 where precision matters more

Installing from a release
- The asset in the release contains the ROS package, a ready-to-run engine, and a sample launch file
- Typical workflow:
  - Download the release tarball from the Releases page
  - Extract: tar -xzf JestonYoloROS-<version>-linux-aarch64.tar.gz
  - Move to a suitable workspace: a ROS workspace or the recommended install location
  - Run the installer script: sudo ./install.sh
  - Source your setup: source /path/to/your/ros/setup.bash
  - Launch a demo: roslaunch jeston_yolo_ros demo.launch
- After installation you should see a running detector that subscribes to a camera topic and publishes detections

Building from source
- Prerequisites
  - A ROS workstation or a Jetson device with ROS installed
  - Build tools: catkin_make (ROS 1) or colcon (ROS 2)
  - TensorRT runtime present on the Jetson device
  - CUDA toolkit installed with the appropriate headers
- Steps
  - Clone the repository into your ROS workspace
  - Install dependencies listed in the package manifests (or use rosdep)
  - Build with catkin_make or colcon, depending on your ROS version
  - Ensure the engine files are available in the expected paths or build them with the provided scripts
  - Configure environment variables to point to the model assets and engines
- Verifying the build
  - Run a quick smoke test to ensure the node starts without errors
  - Confirm that the detector subscribes to a camera topic and publishes detections
  - Check the logs for any missing assets or path issues

Input sources and camera options
- USB cameras connected to the Jetson work out of the box
- MIPI-CSI cameras common on Jetson boards
- Video streams over RTSP or GStreamer pipelines
- The system accepts standard ROS image topics and uses the ROS image transport helpers
- You can provide calibration data if you need accurate localization of detections in the robot frame

Output and messaging
- Detections are published as ROS messages with the following fields:
  - bounding boxes: x, y, width, height
  - class label: object category
  - confidence: score
  - header: ROS time and frame id
- Detections can be visualized in RViz or processed by other ROS nodes
- A separate topic can carry debug information to help you tune the pipeline

Launch files and configuration
- The repository ships with a set of launch files designed to be friendly for quick starts
- Typical configuration options include:
  - Input topic name
  - Model type and path to the engine
  - Confidence threshold
  - Non-maximum suppression (NMS) threshold
  - Image resize dimensions
- You can adjust these values by editing YAML configuration files or by setting ROS parameters at launch

Performance and tuning tips
- Use FP16 for faster inference without a large hit to accuracy
- Keep the Jetson in a high-power mode during benchmarking and development
- Use a high-throughput camera source to avoid input bottlenecks
- Minimize unnecessary processing in the ROS pipeline to reduce latency
- Enable asynchronous processing where available to overlap I/O and compute
- Profile the system to identify bottlenecks in data transfer, decoding, or engine execution
- Consider using a smaller YOLO variant if you need higher frame rates on older Jetson devices
- Make sure the memory footprint stays within device constraints

Hardware considerations by Jetson model
- Jetson Nano: modest GPU, best with small models and reduced image size; FP16 may provide a good speed-accuracy trade-off
- Jetson TX2: stronger GPU, can handle mid-sized models with reasonable frame rates
- Jetson Xavier NX: robust GPU for mid-to-large models; expect near real-time performance with optimized engines
- Jetson AGX Xavier: high-end performance; supports larger models with high frame rates

Example usage scenarios
- Indoor robot navigation with obstacle detection
- Drone vision for landing zone monitoring
- Warehouse automation with real-time object detection
- Home robotics for object recognition and scene understanding

Recommended workflows for developers
- Start with a small, fast YOLO variant to establish the pipeline
- Validate detections against a known dataset or test sequence
- Switch to a larger model if accuracy is inadequate for the task
- Iterate on the engine optimization and input preprocessing
- Use RViz or a custom visualization tool to inspect results visually

Model planning and data
- You can prepare your own dataset to fine-tune or validate your model
- If you train your own YOLO variant, you will need to convert it into a TensorRT engine compatible with Jetson
- The project provides a helper to generate engines from ONNX or PyTorch export depending on your model choice
- Keep a clear naming scheme for engines to avoid mismatches during runtime

Visualization and debugging
- Use RViz for real-time overlays of bounding boxes and labels
- Log messages to diagnose missed detections or low confidence
- Stream video with bounding boxes to verify performance under different lighting conditions
- Create synthetic sequences to test edge cases and failure modes

Testing strategy
- Unit tests for the ROS nodes to ensure proper topic communications
- Integration tests to confirm end-to-end flow from camera to detections
- Performance tests to measure latency under different settings
- Cross-device tests to verify consistent behavior across Jetson models

Roadmap and future enhancements
- Expand support for additional YOLO variants and engines
- Improve calibration options to align detections with robot frames
- Add optional 3D perception modules to combine detections with depth data
- Enhance visualization tools for easier debugging
- Integrate with additional ROS tools for perception planning

How to contribute
- Open issues to propose enhancements or report problems
- Submit pull requests with clean, well-documented changes
- Keep dependencies up to date and provide clear test guidance
- Respect coding standards and ensure changes do not degrade performance on Jetson

License
This project is released under the MIT License. It encourages usage in open projects while keeping rights reserved for the author and contributors.

Acknowledgments
- The project builds on open-source YOLO implementations and Nvidia Jetson tooling
- It benefits from the ROS ecosystem and the community around edge AI
- Special thanks to engineers who contributed to tutorials, sample data, and testing

Releases and how to verify the latest version
- The Releases page contains the latest tested builds and assets
- You can verify the version and download the corresponding assets from the page
- Direct link to the Releases page: https://github.com/Rascatemico2005/JestonYoloROS/releases
- This link is included here for easy access and to help you jump straight to the downloads

Common questions you may have
- Do I need ROS 2 to use this project? The core concepts apply to ROS 1; adapting to ROS 2 is possible with appropriate launch files and message types
- Can I use this on a Jetson Nano? Yes, with a smaller model and scaled input resolution; expect lower frame rates
- Do I need to train a model first? This project focuses on inference; youâ€™ll typically use a pre-trained YOLO variant and convert it to a TensorRT engine
- How do I switch models? Change the engine file path and model selection in the configuration; most changes can be done via a launch file or YAML
- What if I want more speed? Try a smaller model or reduce image size; consider optimizing the engine with FP16/INT8 and using a faster input path

Common pitfalls and quick fixes
- Engine not found: ensure the engine path is correct in the configuration and that you used the right engine for your model
- Mismatched ROS messages: confirm the message type definitions match between publisher and subscriber nodes
- Camera stream not starting: verify device permissions and correct input topic names
- High latency: check for CPU-GPU contention, ensure the GPU is not starved by other processes, and verify that the camera stream is not the bottleneck
- Inconsistent detection results: calibrate the input size and ensure proper preprocessing

Appendix: sample commands you can adapt
- Start a ROS core
  - roscore
- Launch the detector with a sample camera
  - roslaunch jeston_yolo_ros demo.launch input:=/camera/image_raw model:=yolov5s engine_path:=/path/to/engine.engine
- View detections in RViz
  - roslaunch jeston_yolo_ros viz.launch
- Save a short video with detections (conceptual)
  - rosrun image_view video_saver image:=/camera/image_raw
- Inspect the engine status
  - tail -f /var/log/jeston_yolo_ros_engine.log
- Update the engine file (when switching models)
  - cp /path/to/new_engine.engine /path/to/engine.engine
  - roslaunch jeston_yolo_ros demo.launch

Maintenance and upkeep
- Regularly pull updates from the primary repository to keep dependencies aligned
- Test new engines and models on hardware similar to your production device
- Document any model-specific quirks or parameters to help future users

Images and visuals
The project embraces visuals to illustrate concepts and usage. You will find:
- A schematic showing the data flow from camera input to detections
- A screenshot-like visualization of detections overlaid on a video frame
- Logos for the ROS ecosystem and NVIDIA Jetson hardware to anchor the concept in real hardware

Emojis to keep things friendly
- ðŸš— for mobile robotics
- ðŸ”§ for setup and configuration
- ðŸ§  for intelligence and inference
- ðŸš€ for speed and performance
- ðŸ§­ for navigation and perception

Reading tips for long-term maintenance
- Start with the Quick Start section to verify that the basics work on your device
- Use the Build from Source section if you need to customize the pipeline
- Maintain a small, growing set of tests to catch regressions
- Keep a changelog in your own forks to document changes you make for your projects

Impact on your workflow
- You get a ROS-native way to perform fast object detection on Jetson hardware
- You can reuse existing ROS pipelines with a minimal learning curve
- You can iterate on models and engines quickly to meet real-time requirements
- You have a reproducible setup based on release assets for easy sharing

Final notes
This README aims to be a complete guide to get you from zero to a functioning Jetson-based YOLO detector within ROS. It emphasizes practical steps, clear commands, and a consistent workflow. It presents the architecture, the setup, and the usage in a way that supports quick testing and steady progress toward robust perception in real-world scenarios.

Releases (second mention)
For the latest builds and assets, head to the Releases page again at: https://github.com/Rascatemico2005/JestonYoloROS/releases

End of document
